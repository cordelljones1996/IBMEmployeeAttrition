# -*- coding: utf-8 -*-
"""Employee Attrition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u0HF5UThYxeJZLLiREFCvzKhAXgMprI-
"""

# Description: This program predicts employee attrition

# Import libraries
import numpy as np
import pandas as pd
import seaborn as sns

# Import Google Drive (data is too large to upload the file itself)
from google.colab import drive
drive.mount('drive')

# Load IBM employee data from the drive 
upload = r'/content/drive/My Drive/Colab Notebooks/IBMemployeeattrition.csv'
df = pd.read_csv(upload)

# Print the first 7 rows of data
df.head(7)

# Get the number of rows and cols of the df
df.shape
# First number represents the number of employees in the df
# Second number represents the number of fields shown for each employee

# Get the column data types
df.dtypes

# Get a count of the empty values for each column
# This gives the missing data for each employee
df.isna().sum()

# Check for any missing / null values in the df
df.isnull().values.any()

# View some statistics 
df.describe()

# Get a count of the number of employees that stayed and left the company
df['Attrition'].value_counts()
# The number of No's shows how many have stayed
# The number of Yes's shows how many have left

# Visualize the number of employees that stayed and left the company
sns.countplot(df['Attrition'])

# Calculate the accurancy in which we want the model to beat
# This is the percentage if we just guessed no for attrition
(1233 - 237) / 1233

# Show the number of employees that left and stayed by age
import matplotlib.pyplot as plt
plt.subplots(figsize=(12,4)) # Makes the plot wider
sns.countplot(x='Age', hue='Attrition', data=df, palette='colorblind')

# Print all of the data types and their uniqe values
for column in df.columns:
  if df[column].dtype == object:
    print(str(column) + ' : '+ str(df[column].unique()))
    print(df[column].value_counts())
    print('______________________________________________')

# Remove useless columns from the df
df = df.drop('Over18', axis = 1)
df = df.drop('EmployeeNumber', axis = 1)
df = df.drop('StandardHours', axis = 1)
df = df.drop('EmployeeCount', axis = 1)

# Get the correlation
df.corr()

# Visualize the correlation
plt.figure(figsize=(14,14))
sns.heatmap(df.corr(), annot=True, fmt= '.0%')

# Transform the data
# Turning non-numerical into numerical col
from sklearn.preprocessing import LabelEncoder

for column in df.columns:
  if df[column].dtype == np.number:
    continue
  df[column] = LabelEncoder().fit_transform(df[column])

# Create a new column 
df['Age_Years'] = df['Age']

# Drop the pre-existing age column
df = df.drop('Age', axis=1)

# Show the data frame
df

# Split the data into independent x and y data sets
X = df.iloc[:, 1:df.shape[1]].values
Y = df.iloc[:, 0].values

# Split the data into 75% training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

# Use the Random Forest Classifier algorithem 
from sklearn.ensemble import RandomForestClassifier
forest = RandomForestClassifier(n_estimators = 10 , criterion = 'entropy', random_state = 0)
forest.fit(X_train, Y_train)

# Get the accuracy on the training data set
forest.score(X_train, Y_train)

# Show the confusion matrix and accuracy score for the model on the test data
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(Y_test, forest.predict(X_test))

TN = cm[0][0] # True Negative
TP = cm[1][1] # True Positive
FN = cm[1][0] # False Negative
FP = cm[0][1] # False Positive

print(cm)
print('Model Testing Accuracy = {}'.format( (TP + TN) / (TP + TN + FN + FP)))
print('Model Testing Accuracy vs. Guessing = {}'.format( ((TP + TN) / (TP + TN + FN + FP)) - ((1233 - 237) / 1233)))